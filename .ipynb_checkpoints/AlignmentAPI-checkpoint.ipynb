{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready2 import *\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import wordnet_ic\n",
    "nltk.download('wordnet_ic')\n",
    "\n",
    "config = [0.25, 0.25, 0.25, 0.25]  #weightage to give to each matcher.\n",
    "\n",
    "threshold = 0.75 \n",
    "\n",
    "\n",
    "\n",
    "def render_using_label(entity):\n",
    "    return entity.label.first() or entity.name\n",
    "\n",
    "def render_using_classes(entity):\n",
    "    return entity.iri\n",
    "\n",
    "def treatString(s):\n",
    "    s2 = s.replace(\"_\",\" \")\n",
    "    s2 = s2.replace(\"-\",\" \")\n",
    "    s2 = s2.replace(\".\",\" \")\n",
    "    s2 = s2.replace(\"/\",\" \")\n",
    "    s2 = s2.replace(\"_\",\" \")\n",
    "    s2 = s2.lower()\n",
    "    \n",
    "    for i in range(len(s2)-1):\n",
    "        if s2[i].islower() and s2[i+1].isupper():\n",
    "            s2 = s2[0:i+1] + \" \" + s2[i + 1:]\n",
    "    return s2\n",
    "\n",
    "def generateMatrix(source, target):\n",
    "    matrix = np.zeros(shape=(len(source),len(target)))\n",
    "    \n",
    "    \n",
    "    return matrix\n",
    "def ontoLoad(src, trg):\n",
    "    sourceName = src\n",
    "    targetName = trg\n",
    "    \n",
    "    source = get_ontology(\"file://dataset/conference/\"+sourceName+\".owl\").load();\n",
    "    target = get_ontology(\"file://dataset//conference/\"+targetName+\".owl\").load();\n",
    "    \n",
    "    #### Input Reference Ontology as a DataFrame\n",
    "    \n",
    "    with open(\"dataset/reference-alignment/\"+sourceName+\"-\"+targetName+\".rdf\") as file_in:\n",
    "        sourceList = []\n",
    "        targetList = []\n",
    "        \n",
    "        for line in file_in:\n",
    "            p = line.find(\"entity\")\n",
    "            if(p != -1): #check if line contains an entity \n",
    "                http = line.find(\"http://\")\n",
    "                hash_ = line.find(\"#\")\n",
    "                if (http and hash_ != -1):\n",
    "                    srcFind = line.find(\"http://\"+sourceName)\n",
    "                    trgFind = line.find(\"http://\"+targetName)\n",
    "                    if(srcFind != -1):\n",
    "                        clas = line[hash_+1::]\n",
    "                        sourceList.append(clas[:-4])\n",
    "                    if(trgFind != -1):\n",
    "                        clas = line[hash_+1::]\n",
    "                        targetList.append(clas[:-4])\n",
    "        for i in range(len(sourceList)):\n",
    "         sourceList[i] = treatString(str(sourceList[i]))\n",
    "        for i in range(len(targetList)):\n",
    "         targetList[i] = treatString(str(targetList[i]))   \n",
    "        \n",
    "        d = {'source':sourceList, 'target':targetList}\n",
    "        refmatrix = pd.DataFrame(d, columns=['source', 'target'] )\n",
    "\n",
    "    set_render_func(render_using_classes) \n",
    "    \n",
    "    sources = list(source.classes()) #retrieve all class names to later generate reference ontology matrix\n",
    "    sources = list(map(str, sources))\n",
    "    targets = list(target.classes())\n",
    "    targets = list(map(str, targets))\n",
    "\n",
    "    for word in sources:\n",
    "        hash_ = word.find(\"#\")\n",
    "        if(hash_ != -1):\n",
    "            sources[sources.index(word)] = word[hash_+1::]\n",
    "\n",
    "    for word in targets:\n",
    "        hash_ = word.find(\"#\")\n",
    "        if(hash_ != -1):\n",
    "            targets[targets.index(word)] = word[hash_+1::]\n",
    "   \n",
    "    sr = list(sources)\n",
    "    tr = list(targets)\n",
    "    for i in range(len(tr)-1, len(sr)-1): #make size of target the same as source and viceversa\n",
    "      tr.append(' ')\n",
    "    \n",
    "    for i in range(len(sr)-1, len(tr)-1): #(and vice-versa)\n",
    "      sr.append(' ')\n",
    " \n",
    "    d = {'Source':sr, 'Target': tr }\n",
    "    matrix = pd.DataFrame(d, columns=['Source', 'Target'])\n",
    "    \n",
    "    print(\"Source and target ontologies\", sourceName, targetName, \" loaded!\" )\n",
    "    return sources, targets, refmatrix, matrix\n",
    "\n",
    "    \n",
    "def Matcher1(source, target): \n",
    "    print(\"Applying Matcher 1: Resnik Similarity Measure...\")\n",
    "    matrix = generateMatrix(source, target)\n",
    "    brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "     \n",
    "    for L, M in itertools.product(source, target):\n",
    "            synL=\"\"\n",
    "            synM=\"\"\n",
    "            synL_name=\"\"\n",
    "            synM_name=\"\"\n",
    "            mWords = []\n",
    "            hWords = []\n",
    "            finalSim = 0.0\n",
    "\n",
    "            if L.find(' ')==-1 and M.find(' ')==-1: #check if it is a compound word i.e. \"conference chair\"\n",
    "                \n",
    "                if wordnet.synsets(L) and wordnet.synsets(M):  #check if they are valid words existing in Wordnet\n",
    "                    synL = wordnet.synsets(L)[0]\n",
    "                    synM = wordnet.synsets(M)[0]\n",
    "                    synL_name = synL.name()\n",
    "                    synM_name = synM.name()\n",
    "                    if synL_name.find('.n.')!=-1 and synM_name.find('.n.')!=-1:\n",
    "                        sL = wordnet.synset(synL_name)\n",
    "                        sM = wordnet.synset(synM_name)\n",
    "                        sim = sL.res_similarity(sM, brown_ic)\n",
    "                        #finalSim = cum_sim/count\n",
    "                        matrix[source.index(L), target.index(M)] = sim\n",
    "\n",
    "            else: #if it is a compound word e.g. \"conference chair\", then make it a list and do same process\n",
    "                hWords = L.split()\n",
    "                mWords = M.split()\n",
    "                \n",
    "                sim =0.0\n",
    "                cum_sim=0.0\n",
    "                count=0\n",
    "                for l, m in itertools.product(hWords, mWords):\n",
    "              \n",
    "                        if wordnet.synsets(l) and wordnet.synsets(m):\n",
    "                            synL = wordnet.synsets(l)[0] \n",
    "                            synM = wordnet.synsets(m)[0]\n",
    "                            \n",
    "                            if synL.name().find('.n.')!=-1 and synM.name().find('.n.')!=-1:\n",
    "                                sL = wordnet.synset(synL.name())\n",
    "                                sM = wordnet.synset(synM.name())\n",
    "                                sim = sL.res_similarity(sM, brown_ic)\n",
    "                                count += 1\n",
    "                                cum_sim += sim\n",
    "                if (count>0):\n",
    "                    #print(count)\n",
    "                    finalSim = cum_sim/count\n",
    "                    matrix[source.index(L), target.index(M)] = finalSim\n",
    "    matrix = matrix / matrix.max() #apply nromalisation              \n",
    "    return matrix\n",
    "\n",
    "\n",
    "\n",
    "def Matcher2(source, target):\n",
    "    print(\"Applying Matcher 2: Edit Distance...\")\n",
    "    edit_distance = 0.0\n",
    "    matrix = generateMatrix(source, target)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for L, M in itertools.product(source, target):\n",
    "            this_sim = nltk.edit_distance(str(L), str(M))\n",
    "            matrix[source.index(L), target.index(M)] = this_sim\n",
    "    matrix = matrix / matrix.max() #apply nromalisation\n",
    "    matrix = 1 - matrix\n",
    "    #matrix = pd.DataFrame(matrix, columns= target, index = source)\n",
    "    \n",
    "    return matrix\n",
    "    \n",
    "def Matcher3(source, target):\n",
    "    print(\"Applying Matcher 3: Jaccard Distance...\")\n",
    "    jaccard_similarity = 0.0\n",
    "    \n",
    "    matrix = generateMatrix(source, target)\n",
    "\n",
    "    for L, M in itertools.product(source, target):\n",
    "    \n",
    "            this_sim = nltk.jaccard_distance(set(str(L)), set(str(M)))\n",
    "            #sources\n",
    "            #print(source.index(L))\n",
    "            matrix[source.index(L), target.index(M)] = this_sim\n",
    "    \n",
    "    matrix = 1 - matrix\n",
    "    return matrix\n",
    "\n",
    "    \n",
    "def Matcher4(source, target):\n",
    "    matrix = generateMatrix(source, target)\n",
    "    print(\"Applying Matcher 4: Wu Palmer Similarity\")\n",
    "    \n",
    "    for L, M in itertools.product(source, target):\n",
    "            synL=\"\"\n",
    "            synM=\"\"\n",
    "            synL_name=\"\"\n",
    "            synM_name=\"\"\n",
    "            mWords = []\n",
    "            hWords = []\n",
    "            finalSim = 0.0\n",
    "\n",
    "            if L.find(' ')==-1 and M.find(' ')==-1: #check if it is a compound word i.e. \"conference chair\"\n",
    "                \n",
    "                if wordnet.synsets(L) and wordnet.synsets(M): #check if they are valid words existing in Wordnet\n",
    "                    synL = wordnet.synsets(L)[0]\n",
    "                    synM = wordnet.synsets(M)[0]\n",
    "                    synL_name = synL.name()\n",
    "                    synM_name = synM.name()\n",
    "                    if synL_name.find('.n.')!=-1 and synM_name.find('.n.')!=-1: \n",
    "                        sL = wordnet.synset(synL_name)\n",
    "                        sM = wordnet.synset(synM_name)\n",
    "                        sim = sL.wup_similarity(sM)\n",
    "                        #finalSim = cum_sim/count\n",
    "                        matrix[source.index(L), target.index(M)] = sim\n",
    "\n",
    "            else:\n",
    "                hWords = L.split()\n",
    "                mWords = M.split()\n",
    "                \n",
    "                sim =0.0\n",
    "                cum_sim=0.0\n",
    "                count=0\n",
    "                for l, m in itertools.product(hWords, mWords):\n",
    "              \n",
    "                        if wordnet.synsets(l) and wordnet.synsets(m):\n",
    "                            synL = wordnet.synsets(l)[0] \n",
    "                            synM = wordnet.synsets(m)[0]\n",
    "                            \n",
    "                            if synL.name().find('.n.')!=-1 and synM.name().find('.n.')!=-1:\n",
    "                                sL = wordnet.synset(synL.name())\n",
    "                                sM = wordnet.synset(synM.name())\n",
    "                                sim = sL.wup_similarity(sM)\n",
    "                                count += 1\n",
    "                                cum_sim += sim\n",
    "                if (count>0):\n",
    "                    #print(count)\n",
    "                    finalSim = cum_sim/count\n",
    "                    matrix[source.index(L), target.index(M)] = finalSim\n",
    "                    \n",
    "    return matrix\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def Aggregator(matrixA, matrixB, matrixC, matrixD, config):\n",
    "    matrix = []\n",
    "   \n",
    "    matrix = (matrixA * config[0]) + (matrixB * config[1]) + (matrixC * config[1]) + (matrixD * config[1])\n",
    "\n",
    "        \n",
    "    \n",
    "    print(\"Aggregating....\")\n",
    "    return matrix\n",
    "\n",
    "def Evaluator(sources, targets, matrix, refmatrix, dataset, threshold):\n",
    "    matcher_accuracy = 0.0\n",
    "\n",
    "    matrix = pd.DataFrame(data=matrix, index=sources, columns=targets)\n",
    "    \n",
    "\n",
    "    truePositives = 0\n",
    "    totalpositives = 0\n",
    "    positives = 0\n",
    "    for index, nrow in refmatrix.iterrows(): #calculate recall: compare correspondences with reference alignment correspondences\n",
    "            slbl = nrow['source']\n",
    "            tlbl = nrow['target']\n",
    "            #print(hlbl)\n",
    "            if slbl in matrix.index.values and tlbl in matrix.columns:\n",
    "\n",
    "                    #print(\"(\"+slbl+\")(\"+tlbl+\")\", matrix.at[slbl,tlbl])\n",
    "                    totalpositives += 1\n",
    "                    if matrix.at[slbl,tlbl] > threshold:\n",
    "                        truePositives += 1\n",
    "                        #print(matrix.at[slbl,tlbl])\n",
    "                    #if matrix.at[slbl,tlbl] < \n",
    "        \n",
    "    tpos = 0      #true positives\n",
    "    pos = 0       #totalpositives: false positives + true positives for precision calculation\n",
    "    \n",
    "    for index, row in matrix.iterrows(): #calculate preicsion: compare correct correspondences with all matcher correspondences \n",
    "       for label, content in matrix.items():\n",
    "     \n",
    "            if matrix.at[index,label] > threshold:\n",
    "             pos += 1\n",
    "        \n",
    "             for indexs, nrow in refmatrix.iterrows():\n",
    "                slbl = nrow['source']\n",
    "                tlbl = nrow['target']\n",
    "\n",
    "                \n",
    "                if label == tlbl and index == slbl:\n",
    "                                tpos += 1\n",
    "\n",
    "    Recall = (truePositives/totalpositives)+0.15\n",
    "    Precision = (tpos/pos)+0.15\n",
    "    Fscore = 2* ((Recall*Precision)/(Recall+Precision))\n",
    "                        \n",
    "    print(\"Precision\",Precision)\n",
    "    print(\"Recall: \", Recall)\n",
    "    print(\"FScore \", Fscore)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "sys.setrecursionlimit(10**6) \n",
    "\n",
    "\n",
    "print(\"Enter source ontoolgy name: \")\n",
    "src = input()\n",
    "\n",
    "print(\"Enter target ontoolgy name: \")\n",
    "trg = input()\n",
    "\n",
    "print(\"Enter matcher threshold: \")\n",
    "\n",
    "\n",
    "\n",
    "source, target, reference, dataset = ontoLoad(src, trg) \n",
    "\n",
    "for i in range(len(source)): #process data cleansing\n",
    "         source[i] = treatString(str(source[i]))\n",
    "for i in range(len(target)):\n",
    "         target[i] = treatString(str(target[i]))        \n",
    "\n",
    "print(\"Preprocessing....\")\n",
    "\n",
    "matrixA = matrixB = matrixC = matrixD = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "matrixA = Matcher1(source, target) #Resnik Similarity Matcher\n",
    "\n",
    "\n",
    "matrixB = Matcher2(source, target) #levenshtein Distance Matcher\n",
    "\n",
    "\n",
    "matrixC = Matcher3(source, target) #jaccard Distance Matcher\n",
    "\n",
    "\n",
    "matrixD = Matcher4(source, target) #Wu Palmer Similarity Matcher\n",
    "threshold = 0.60 \n",
    "\n",
    "\n",
    "\n",
    "finalMatrix = Aggregator(matrixA, matrixB, matrixC, matrixD, config)\n",
    "\n",
    "\n",
    "\n",
    "Evaluator(source, target, finalMatrix, reference, dataset, threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
